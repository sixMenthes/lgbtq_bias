- QueerBench: https://arxiv.org/html/2406.12399. 

- HeteroCorpus: A Corpus for Heteronormative Language Detection: https://aclanthology.org/2022.gebnlp-1.23/. 

- WinoQueer: https://arxiv.org/abs/2306.15087

- Gender Shades: https://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf

- Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals: https://iris.unibocconi.it/bitstream/11565/4052801/1/2022.ltedi-1.4.pdf

- Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image ModelsStereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models: https://arxiv.org/abs/2305.17072

- Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation: https://arxiv.org/abs/2401.04972

- Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models: https://arxiv.org/abs/2405.14555

- Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models: https://arxiv.org/abs/2307.00101

- Some Myths About Bias: A Queer Studies Reading Of Gender Bias In NLP: https://aclanthology.org/2025.gebnlp-1.29/

- Transphobia is in the eye of the prompter: https://dl.acm.org/doi/pdf/10.1145/3743676

- Ensemble modeling for homophobia and transphobia detection: https://aclanthology.org/2022.ltedi-1.37.pdf

# A Bayesian account of pronoun and neopronoun acquisition
https://aclanthology.org/2025.queerinai-main.5.pdf

Researches propose a statistical model to explain usage of pronouns when adressing a speaker. Instead of treating meaning as a mapping from a symbolic feature to a referent, they treat the speakers as "documents" whose symbolic spectra can be modelled by a Latent Dirichlet Allocation. Hence the usage of pronouns would be determined by the cultural and pragmatic priors involved in the utterance. This is a nice fragment useful for our research, with a few mentions worth checking:

"A challenge for modeling pronoun use in practical
systems arises when we presuppose that learning
words boils down to the problem of mapping form
onto meaning. For instance, early connectionist approaches to semantic representation, have treated
the "meaning" of a word as a sparse d-dimensional
vector consisting of several manually-selected semantic features (Cree et al., 2006; Rumelhart et al.,
1986). Here, we propose that meaning be defined
symbolically at the level of a referent rather than
distributed across semantic features.
In word vectors trained on corpora, a “gender
subspace” commonly emerges (Bolukbasi et al.,
2016) that encodes social biases about canonical genders (e.g., stereotypes about the gender of
nurses versus doctors). Pronouns and other highfrequency gendered nouns (e.g., man, woman) typically serve as critical anchors in the debiasing process, and serve as an excellent probe into the origins of biases in modern statistical NLP systems.
Others have successfully demonstrated that nonbinary pronoun LLM representations can be debiased, suggesting that the form-to-meaning mapping
can be partially undone for novel referential forms
(van Boven et al., 2024)."


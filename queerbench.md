# QueerBench: Quantifying Discrimination in Language
Models Toward Queer Identities

## Abstract:

 In this paper, we assess the potential
harm caused by sentence completions generated by English large language models (LLMs)
concerning LGBTQIA+ individuals. This is
achieved using QueerBench, our new assessment framework which employs a templatebased approach and a Masked Language Modeling (MLM) task. The analysis indicates that
large language models tend to exhibit discriminatory behaviour more frequently towards individuals within the LGBTQIA+ community,
reaching a difference gap of 7.2% in the QueerBench score of harmfulness.


## Dataset:

The method to build the dataset is having a set of neutral sentence templates, and a set of subjects, and combining them to generate a sentence. Then, one token is masked, and the probability distribution calculated by the model for that masked token is evaluated. 

On subjects:

There are two types of subjects: nouns
and pronouns. In the case of the nouns group, all
words are categorized as either “queer” or “nonqueer”. These categories further include word
related to “sexual orientation”, “gender identity”,
and “other”. The set of pronouns is classified into
the categories “neutral pronouns”, “neo-pronouns”,
and “binary pronouns”.

## Assessment metrics:

AFINN. Sentiment analysis tool. To be noted: this metric yielded similar values, all close to 0, for either category of pronouns. 
Hurtlex. multilingual lexicon of hate words composed of derogatory words
Perspective API. a tool that employs machine learning to detect toxic comments sentence-based

## To remember:
